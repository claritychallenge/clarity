

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The First Cadenza Challenge (CAD1) - Task 1: Listening music via headphones &mdash; pyClarity 0.7.1.post21 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css" />

  
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=5cc28f93"></script>
      <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            pyClarity
              <img src="../../../../_static/challenges.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">pyClarity</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The First Cadenza Challenge (CAD1) - Task 1: Listening music via headphones</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/recipes/cad1/task1/baseline/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-first-cadenza-challenge-cad1-task-1-listening-music-via-headphones">
<h1>The First Cadenza Challenge (CAD1) - Task 1: Listening music via headphones<a class="headerlink" href="#the-first-cadenza-challenge-cad1-task-1-listening-music-via-headphones" title="Link to this heading"></a></h1>
<p>Cadenza challenge code for the First Cadenza Challenge (CAD1) Task1.</p>
<p>For more information please visit the <a class="reference external" href="https://cadenzachallenge.org/docs/cadenza1/cc1_intro">challenge website</a>.</p>
<section id="data-structure">
<h2>1. Data structure<a class="headerlink" href="#data-structure" title="Link to this heading"></a></h2>
<p>The First Cadenza Challenge - task 1 is using the MUSDB18-HQ dataset.
The data is split into train, validation and test following the same split from museval.
I.e., 86 songs are for training, 16 for validation and 50 for evaluation.</p>
<p>To download the data, please visit <a class="reference external" href="https://forms.gle/UQkuCxqQVxZtGggPA">here</a>. The data is split into <code class="docutils literal notranslate"><span class="pre">cadenza_cad1_task1_core_musdb18hq.tar.gz</span></code> (containing the MUSDB18-HQ dataset) and
<code class="docutils literal notranslate"><span class="pre">cadenza_cad1_task1_core_metadata.tar.gz</span></code> (containing the list of songs and listeners’ characteristics per split).
Alternatively, you can download the MUSDB18-HQ dataset from the official <a class="reference external" href="https://sigsep.github.io/datasets/musdb.html#musdb18-hq-uncompressed-wav">SigSep website</a>.
If you opt for this alternative, be sure to download the uncompressed wav version. Note that you will need both packages to run the baseline system.</p>
<p>If you need additional music data for training your model, please restrict to the use of <a class="reference external" href="https://medleydb.weebly.com/">MedleyDB</a> [<a class="reference internal" href="#4-references"><span class="xref myst">4</span></a>] [<a class="reference internal" href="#4-references"><span class="xref myst">5</span></a>],
<a class="reference external" href="https://labsites.rochester.edu/air/resource.html">BACH10</a> [<a class="reference internal" href="#4-references"><span class="xref myst">6</span></a>] and <a class="reference external" href="https://github.com/mdeff/fma">FMA-small</a> [<a class="reference internal" href="#4-references"><span class="xref myst">7</span></a>].
Theses are shared as <code class="docutils literal notranslate"><span class="pre">cadenza_cad1_task1_augmentation_medleydb.tar.gz</span></code>, <code class="docutils literal notranslate"><span class="pre">cadenza_cad1_task1_augmentation_bach10.tar.gz</span></code>
and <code class="docutils literal notranslate"><span class="pre">cadenza_cad1_task1_augmentation_fma_small.tar.gz</span></code>.
<strong>Keeping the augmentation data restricted to these datasets will ensure that the evaluation is fair for all participants</strong>.</p>
<p>Unpack packages under the same root directory using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-xvzf<span class="w"> </span>&lt;PACKAGE_NAME&gt;
</pre></div>
</div>
<section id="necessary-data">
<h3>1.1 Necessary data<a class="headerlink" href="#necessary-data" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Music</strong> contains the MUSDB18-HQ music dataset for training, validation and evaluation.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───task1
    └───audio
        └───musdb18hq
            ├───train
            └───test
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Metadata</strong> contains the metadata for the systems.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───task1
    └───metadata
        └───musdb18hq
            ├───listeners.train.json
            ├───listeners.valid.json
            ├───musdb18.train.json
            ├───musdb18.valid.json
            └───musdb18.test.json
</pre></div>
</div>
</section>
<section id="additional-optional-data">
<h3>1.2 Additional optional data<a class="headerlink" href="#additional-optional-data" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>MedleyDB</strong> contains both MedleyDB versions 1 [<a class="reference internal" href="#4-references"><span class="xref myst">4</span></a>] and 2 [<a class="reference internal" href="#4-references"><span class="xref myst">5</span></a>] datasets.</p></li>
</ul>
<p>Tracks from the MedleyDB dataset are not included in the evaluation set.
However, is your responsibility to exclude any song that may be already contained in the training set.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───task1
    └───audio
        └───MedleyDB
            ├───Audio
            └───Metadata
</pre></div>
</div>
<ul class="simple">
<li><p><strong>BACH10</strong> contains the BACH10 dataset [<a class="reference internal" href="#4-references"><span class="xref myst">6</span></a>].</p></li>
</ul>
<p>Tracks from the BACH10 dataset are not included in MUSDB18-HQ and can all be used as training augmentation data.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───task1
    └───audio
        └───fma_small
            ├───000
            ├───001
            ├───...
</pre></div>
</div>
<ul class="simple">
<li><p><strong>FMA Small</strong> contains the FMA small subset of the FMA dataset [<a class="reference internal" href="#4-references"><span class="xref myst">7</span></a>].</p></li>
</ul>
<p>Tracks from the FMA small dataset are not included in the MUSDB18-HQ.
This dataset does not provide independent stems but only the full mix.
However, it can be used to train an unsupervised model to better initialise a supervised model.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───task1
    └───audio
        └───fma_small
            ├───000
            ├───001
            ├───...
</pre></div>
</div>
</section>
<section id="demo-data">
<h3>1.3 Demo data<a class="headerlink" href="#demo-data" title="Link to this heading"></a></h3>
<p>To help you to start with the challenge, we provide a small subset of the data.
The <code class="docutils literal notranslate"><span class="pre">demo_data</span></code> folder contains a single song and two listeners from the validation set.</p>
<p>To use the demo data, simply download the package <code class="docutils literal notranslate"><span class="pre">cadenza_data_demo.tar.xz</span></code>
from <a class="reference external" href="https://drive.google.com/drive/folders/1Yxo_R-yPByEUvX5O5lhsHk3tW1ek5qKW?usp=share_link">here</a>
and unpack it under <code class="docutils literal notranslate"><span class="pre">recipes/cad1/task1/</span></code>, i.e., one level above the baseline directory.
Note that the <code class="docutils literal notranslate"><span class="pre">root.path</span></code> variable in <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> is already set to the demo data by default.</p>
<p>To unpack the demo data, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-xvf<span class="w"> </span>cadenza_data_demo.tar.xz
</pre></div>
</div>
</section>
</section>
<section id="baseline">
<h2>2. Baseline<a class="headerlink" href="#baseline" title="Link to this heading"></a></h2>
<p>In the <code class="docutils literal notranslate"><span class="pre">baseline/</span></code> folder, we provide code for running the baseline enhancement system and performing the objective evaluation.
Note that we use <a class="reference external" href="https://hydra.cc/docs/intro/">hydra</a> for config handling.</p>
<section id="enhancement">
<h3>2.1 Enhancement<a class="headerlink" href="#enhancement" title="Link to this heading"></a></h3>
<p>We offer two baseline systems:</p>
<ol class="arabic simple">
<li><p>Using the out-of-the-box time-domain <a class="reference external" href="https://github.com/facebookresearch/demucs">Hybrid Demucs</a> [<a class="reference internal" href="#4-references"><span class="xref myst">1</span></a>]
source separation model distributed on <a class="reference external" href="https://pytorch.org/audio/main/tutorials/hybrid_demucs_tutorial.html">TorchAudio</a></p></li>
<li><p>Using the out-of-the-box spectrogram-based <a class="reference external" href="https://github.com/sigsep/open-unmix-pytorch">Open-Unmix</a>
source separation model (version <code class="docutils literal notranslate"><span class="pre">umxhq</span></code>) distributed through <a class="reference external" href="https://pytorch.org/hub/">PyTorch Hub</a></p></li>
</ol>
<p>Both system use the same enhancement strategy; using the music separation model, the baseline system estimates the
VDBO (<code class="docutils literal notranslate"><span class="pre">vocals</span></code>, <code class="docutils literal notranslate"><span class="pre">drums</span></code>, <code class="docutils literal notranslate"><span class="pre">bass</span></code> and <code class="docutils literal notranslate"><span class="pre">others</span></code>) stems. Then, they apply a simple NAL-R [<a class="reference internal" href="#4-references"><span class="xref myst">2</span></a>] fitting amplification to each of them.
These results on eight mono signals (four from the left channel and four from the right channel). Finally, each signal is downsampled to 24000 Hertz, convert to 16bit precision and
encoded using the lossless FLAC compression. These eight signal are then used for the objective evaluation (HAAQI).</p>
<p>The baselines also provide a remixing strategy to generate a stereo signal for each listener. This is done by summing
the amplified VDBO stems, where each channel (left and right in stereo) is composed of the addition of the corresponding
four stems. This stereo remixed signal is then used for subjective evaluation (listening panel).</p>
<p>To run the baseline enhancement system first, make sure that <code class="docutils literal notranslate"><span class="pre">paths.root</span></code> in <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> points to
where you have installed the Cadenza data. This parameter defaults to the working directory.
You can also define your own <code class="docutils literal notranslate"><span class="pre">path.exp_folder</span></code> to store the enhanced signals and evaluated results and select what
music separation model you want to employ.</p>
<p>Then run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py
</pre></div>
</div>
<p>Alternatively, you can provide the root variable on the command line, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>path.root<span class="o">=</span>/full/path/to/my/cadenza_data
</pre></div>
</div>
<p>To get a full list of the parameters, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>--help
</pre></div>
</div>
<p>The folder <code class="docutils literal notranslate"><span class="pre">enhanced_signals</span></code> will appear in the <code class="docutils literal notranslate"><span class="pre">exp</span></code> folder.</p>
</section>
<section id="evaluation">
<h3>2.2 Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluate.py</span></code> script takes the eight VDBO signals stored in <code class="docutils literal notranslate"><span class="pre">enhanced_signals</span></code> and computes the
HAAQI [<a class="reference internal" href="#4-references"><span class="xref myst">3</span></a>] score. The final score for the sample is the average of the scores of each stem.</p>
<p>To run the evaluation stage, make sure that <code class="docutils literal notranslate"><span class="pre">path.root</span></code> is set in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file and then run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py
</pre></div>
</div>
<p>A csv file containing the eight HAAQI scores and the combined score will be generated in the <code class="docutils literal notranslate"><span class="pre">path.exp_folder</span></code>.</p>
<p>To check the HAAQI code, see <a class="reference internal" href="#../../../../clarity/evaluator/haaqi"><span class="xref myst">here</span></a>.</p>
<p>Please note: you will not get identical HAAQI scores for the same signals if the random seed is not defined.
This is due to the  random noises generated within HAAQI, but the differences should be sufficiently small.
For reproducibility, in the given recipe, the random seed for each signal is set as the last eight digits
of the song md5.</p>
</section>
</section>
<section id="results">
<h2>3. Results<a class="headerlink" href="#results" title="Link to this heading"></a></h2>
<p>The overall HAAQI score for each baseline is:</p>
<ul class="simple">
<li><p>Demucs: <strong>0.2592</strong></p></li>
<li><p>Open-Unmix: <strong>0.2273</strong></p></li>
</ul>
</section>
<section id="references">
<h2>4. References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>[1] Défossez, A. “Hybrid Spectrogram and Waveform Source Separation”. Proceedings of the ISMIR 2021 Workshop on Music Source Separation. <a class="reference external" href="https://arxiv.org/abs/2111.03600">doi:10.48550/arXiv.2111.03600</a></p></li>
<li><p>[2] Byrne, Denis, and Harvey Dillon. “The National Acoustic Laboratories’(NAL) new procedure for selecting the gain and frequency response of a hearing aid.” Ear and hearing 7.4 (1986): 257-265. <a class="reference external" href="https://doi.org/10.1097/00003446-198608000-00007">doi:10.1097/00003446-198608000-00007</a></p></li>
<li><p>[3] Kates J M, Arehart K H. “The Hearing-Aid Audio Quality Index (HAAQI)”. IEEE/ACM transactions on audio, speech, and language processing, 24(2), 354–365. <a class="reference external" href="https://doi.org/10.1109%2FTASLP.2015.2507858">doi:10.1109/TASLP.2015.2507858</a></p></li>
<li><p>[4] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, “MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research”, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014. <a class="reference external" href="https://archives.ismir.net/ismir2014/paper/000322.pdf">pdf</a></p></li>
<li><p>[5] Rachel M. Bittner, Julia Wilkins, Hanna Yip and Juan P. Bello, “MedleyDB 2.0: New Data and a System for Sustainable Data Collection” Late breaking/demo extended abstract, 17th International Society for Music Information Retrieval (ISMIR) conference, August 2016. <a class="reference external" href="https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/bittner-medleydb.pdf">pdf</a></p></li>
<li><p>[6] Zhiyao Duan, Bryan Pardo and Changshui Zhang, “Multiple fundamental frequency estimation by modeling spectral peaks and non-peak regions,” IEEE Trans. Audio Speech  Language Process., vol. 18, no. 8, pp. 2121-2133, 2010. <a class="reference external" href="https://doi.org/10.1109/TASL.2010.2042119">doi:10.1109/TASL.2010.2042119</a></p></li>
<li><p>[7] Defferrard, M., Benzi, K., Vandergheynst, P., &amp; Bresson, X. (2016). “FMA: A dataset for music analysis”. arXiv preprint arXiv:1612.01840. <a class="reference external" href="https://doi.org/10.48550/arXiv.1612.01840">doi:10.48550/arXiv.1612.01840</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2025, pyClarity authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>