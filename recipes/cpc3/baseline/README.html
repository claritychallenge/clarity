

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The 3rd Clarity Prediction Challenge (CPC3) baseline code &mdash; pyClarity 0.7.1.post21 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=5cc28f93"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            pyClarity
              <img src="../../../_static/challenges.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">pyClarity</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The 3rd Clarity Prediction Challenge (CPC3) baseline code</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/recipes/cpc3/baseline/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-3rd-clarity-prediction-challenge-cpc3-baseline-code">
<h1>The 3rd Clarity Prediction Challenge (CPC3) baseline code<a class="headerlink" href="#the-3rd-clarity-prediction-challenge-cpc3-baseline-code" title="Link to this heading"></a></h1>
<p>Code to support the 3rd Clarity Prediction Challenge (CPC3).</p>
<p>For more information about the CPC3 please <a class="reference external" href="https://claritychallenge.org/">visit</a></p>
<section id="data-structure">
<h2>1. Data structure<a class="headerlink" href="#data-structure" title="Link to this heading"></a></h2>
<section id="obtaining-the-cpc3-data">
<h3>1.1 Obtaining the CPC3 data<a class="headerlink" href="#obtaining-the-cpc3-data" title="Link to this heading"></a></h3>
<p>To download the CPC3 data, please follow the instructions on the <a class="reference external" href="https://claritychallenge.org/docs/cpc3/taking_part/cpc3_download">download page</a> of the challenge website.</p>
<p>The data will download into two package files called <code class="docutils literal notranslate"><span class="pre">clarity_CPC3_data.v1_1.tar.gz</span></code> and `clarity_CPC3_data.dev.v1_0.tar.gz.</p>
<p>Unpack these packages under the same root using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-xvzf<span class="w"> </span>clarity_CPC3_data.v1_1.tar.gz
tar<span class="w"> </span>-xvzf<span class="w"> </span>clarity_CPC3_data.dev.v1_0.tar.gz
</pre></div>
</div>
<p>(Ignore any warnings about overwriting files, this is expected.)</p>
<p>Once unpacked the directory structure will be as follows</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>clarity_CPC3_data/
├──<span class="w"> </span>clarity_data/
│<span class="w">   </span>├──<span class="w"> </span>metadata/<span class="w">  </span><span class="c1"># Listener responses and characteristics</span>
│<span class="w">   </span>├──<span class="w"> </span>train/
│<span class="w">   </span>│<span class="w">   </span>├──<span class="w"> </span>references/<span class="w">  </span><span class="c1"># Reference signals for intelligibility prediction</span>
│<span class="w">   </span>│<span class="w">   </span>└──<span class="w"> </span>signals/<span class="w">     </span><span class="c1"># Hearing aid output signals</span>
│<span class="w">   </span>└──<span class="w"> </span>dev/
│<span class="w">       </span>├──<span class="w"> </span>references/
│<span class="w">       </span>└──<span class="w"> </span>signals/
└──<span class="w"> </span>manifest/
</pre></div>
</div>
</section>
<section id="demo-data">
<h3>1.2 Demo data<a class="headerlink" href="#demo-data" title="Link to this heading"></a></h3>
<p>Running the baseline code over the full dataset can be quite time consuming. To allow you to easily try out the code with a small amount of data, we have provided a small subset of demo data containing 30 signals.</p>
<p>To use the demo data download the file <code class="docutils literal notranslate"><span class="pre">clarity_CPC3_demo_data.v1_0.tar.gz</span></code> which can be found at the same download sites as the full dataset. (<a class="reference external" href="https://claritychallenge.org/docs/cpc3/cpc3_download">See our website</a> for details of how to obtain the data.)</p>
<p>Unpack this package under this directory, i.e., under <code class="docutils literal notranslate"><span class="pre">recipes/cpc3/baseline</span></code> using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-xvzf<span class="w"> </span>clarity_CPC3_demo_data.v1_0.tar.gz
</pre></div>
</div>
<p>Note, the <code class="docutils literal notranslate"><span class="pre">root.path</span></code> variable in <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> is already set to point to the demo data by default.</p>
<p>The demo data only contains has the following structure,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>clarity_CPC3_data/
├──<span class="w"> </span>clarity_demo_data/
│<span class="w">   </span>├──<span class="w"> </span>metadata/<span class="w">  </span><span class="c1"># Listener responses and characteristics</span>
│<span class="w">   </span>└───<span class="w"> </span>train/
│<span class="w">       </span>├──<span class="w"> </span>references/<span class="w">  </span><span class="c1"># Reference signals for intelligibility prediction</span>
│<span class="w">       </span>└──<span class="w"> </span>signals/
└──<span class="w"> </span>manifest/
</pre></div>
</div>
</section>
<section id="precomputed-haspi-scores">
<h3>1.3 Precomputed HASPI scores<a class="headerlink" href="#precomputed-haspi-scores" title="Link to this heading"></a></h3>
<p>For convenience, precomputed HASPI scores are provided in the <code class="docutils literal notranslate"><span class="pre">precomputed_haspi</span></code> directory. These scores are stored in JSONL files and can be used directly without running the <code class="docutils literal notranslate"><span class="pre">compute_haspi.py</span></code> script. The files are named as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">precomputed_haspi/clarity_data.train.haspi.jsonl</span></code>: HASPI scores for the full training dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">precomputed_haspi/clarity_demo_data.train.haspi.jsonl</span></code>: HASPI scores for the demo training dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">precomputed_haspi/clarity_data.dev.haspi.jsonl</span></code>: HASPI scores for the development dataset.</p></li>
</ul>
<p>To use these precomputed scores, copy the relevant file to the <code class="docutils literal notranslate"><span class="pre">exp/</span></code> directory and rename it to match the expected output of the <code class="docutils literal notranslate"><span class="pre">compute_haspi.py</span></code> script. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp<span class="w"> </span>precomputed_haspi/clarity_data.train.haspi.jsonl<span class="w"> </span>exp/clarity_data.train.haspi.jsonl
</pre></div>
</div>
<p>This allows you to skip the HASPI computation step and proceed directly to making intelligibility predictions.</p>
</section>
</section>
<section id="baseline-evaluating-with-training-data">
<h2>2. Baseline - evaluating with training data<a class="headerlink" href="#baseline-evaluating-with-training-data" title="Link to this heading"></a></h2>
<p>The baseline prediction model is a simple logistic regression model that maps HASPI scores [<a class="reference internal" href="#references"><span class="xref myst">1</span></a>] onto the sentence correctness values. It can be run using and evaluated on the training data set (this section), or using the development data set (see the next section).</p>
<section id="computing-the-haspi-scores">
<h3>2.1 Computing the HASPI scores<a class="headerlink" href="#computing-the-haspi-scores" title="Link to this heading"></a></h3>
<p>To compute the HASPI scores for the training data set, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>compute_haspi.py<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>clarity_data<span class="w"> </span><span class="nv">split</span><span class="o">=</span>train
</pre></div>
</div>
<p>If you are using the demo dataset, replace <code class="docutils literal notranslate"><span class="pre">clarity_data</span></code> with <code class="docutils literal notranslate"><span class="pre">clarity_demo_data</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>compute_haspi.py<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>clarity_demo_data<span class="w"> </span><span class="nv">split</span><span class="o">=</span>train
</pre></div>
</div>
<p>This will generate an output file containing the HASPI scores. The file will be saved in the <code class="docutils literal notranslate"><span class="pre">exp/</span></code> directory and named <code class="docutils literal notranslate"><span class="pre">&lt;DATASET&gt;.train.haspi.jsonl</span></code>, where <code class="docutils literal notranslate"><span class="pre">&lt;DATASET&gt;</span></code> is the dataset name specified in the command (e.g., <code class="docutils literal notranslate"><span class="pre">clarity_data</span></code> or <code class="docutils literal notranslate"><span class="pre">clarity_demo_data</span></code>).</p>
<p>For example:</p>
<ul class="simple">
<li><p>For the full dataset: <code class="docutils literal notranslate"><span class="pre">exp/clarity_data.train.haspi.jsonl</span></code></p></li>
<li><p>For the demo dataset: <code class="docutils literal notranslate"><span class="pre">exp/clarity_demo_data.train.haspi.jsonl</span></code></p></li>
</ul>
<p>If the results file already exists, the script will only compute scores for signals that are not already present in the file. Missing results will be appended to the existing file. This allows the script to be halted and restarted without losing progress.</p>
<p>Note: The <code class="docutils literal notranslate"><span class="pre">exp/</span></code> directory will be created automatically if it does not already exist. Ensure you have write permissions in the current directory.</p>
</section>
<section id="making-intelligibility-predictions">
<h3>2.2 Making intelligibility predictions<a class="headerlink" href="#making-intelligibility-predictions" title="Link to this heading"></a></h3>
<p>The baseline intelligibility predictions are made by using a logistic fitting to map the HASPI scores onto the sentence correctness values. This is done using <code class="docutils literal notranslate"><span class="pre">predict_train.py</span></code>, which will produce a CSV file named <code class="docutils literal notranslate"><span class="pre">exp/&lt;DATASET&gt;.train.predict.csv</span></code> containing the predictions in the format required for submission to the challenge.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>predict_train.py<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>clarity_data
</pre></div>
</div>
<p>Note that the algorithm avoids overfitting by ensuring that the predictions for a signal are formed from HASPI scores from a disjoint subset of the training data. Specifically, this set is formed by removing any signals that have the same target sentence, listener or HA system as the signal for which the prediction is being made. This helps to ensure that the performance will be representative of the performance on the disjoint development and test sets.</p>
</section>
<section id="evaluating-the-predictions">
<h3>2.3 Evaluating the predictions<a class="headerlink" href="#evaluating-the-predictions" title="Link to this heading"></a></h3>
<p>Finally, the <code class="docutils literal notranslate"><span class="pre">evaluate.py</span></code> script will compare the provided predictions with the ground truth and compute the error metrics.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>clarity_demo_data<span class="w"> </span><span class="nv">split</span><span class="o">=</span>train
</pre></div>
</div>
<p>Results will be displayed on the terminal and saved to the file <code class="docutils literal notranslate"><span class="pre">exp/&lt;DATASET&gt;.evaluate.jsonl</span></code>.</p>
</section>
</section>
<section id="baseline-evaluating-with-development-data">
<h2>3. Baseline - evaluating with development data<a class="headerlink" href="#baseline-evaluating-with-development-data" title="Link to this heading"></a></h2>
<p>The baseline prediction model can also be run over the development data set. The development set ground truth is not provided, but once predictions are made they can be submitted to the leaderboard for evaluation. See the challenge website for details of how to submit your predictions.</p>
<section id="id1">
<h3>3.1 Computing the HASPI scores<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>This is done identically to the training data set, but using the <code class="docutils literal notranslate"><span class="pre">split=dev</span></code> option.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>compute_haspi.py<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>clarity_data<span class="w"> </span><span class="nv">split</span><span class="o">=</span>dev
</pre></div>
</div>
<p>This will generate the output file containing HASPI scores. This file will be called <code class="docutils literal notranslate"><span class="pre">exp/&lt;DATASET&gt;.dev.haspi.jsonl</span></code> where <code class="docutils literal notranslate"><span class="pre">&lt;DATASET&gt;</span></code> is the name specified on the commandline.</p>
</section>
<section id="id2">
<h3>3.2 Making intelligibility predictions<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<p>The baseline intelligibility predictions for the development set use a logistic fitting to map the HASPI scores onto the sentence correctness values. This is done using <code class="docutils literal notranslate"><span class="pre">predict_dev.py</span></code>, which will produce a CSV file named <code class="docutils literal notranslate"><span class="pre">exp/&lt;DATASET&gt;.dev.predict.csv</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>predict_dev.py<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>clarity_data
</pre></div>
</div>
<p>These predictions are intended for submission to the leaderboard for evaluation. See the challenge website for details on how to submit your predictions.</p>
</section>
<section id="id3">
<h3>3.3 Evaluating the predictions<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<p>Unlike for the training data set, the development set ground truth is not provided, but once predictions are made they can be submitted to the leaderboard for evaluation. See the challenge website for details of how to submit your predictions.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p>[1] Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (HASPI) version 2. Speech Communication, 131, pp.35-46.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2025, pyClarity authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>