<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The ICASSP 2024 Cadenza Challenge (CAD_ICASSP_2024) &#8212; Project name not set  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=12dfc556" />
    <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="the-icassp-2024-cadenza-challenge-cad-icassp-2024">
<h1>The ICASSP 2024 Cadenza Challenge (CAD_ICASSP_2024)<a class="headerlink" href="#the-icassp-2024-cadenza-challenge-cad-icassp-2024" title="Link to this heading">¶</a></h1>
<p>Cadenza challenge code for the ICASSP 2024 Cadenza Challenge.</p>
<p>For more information please visit the <a class="reference external" href="https://cadenzachallenge.org/docs/icassp_2024/intro">challenge website</a>.</p>
<section id="data-structure">
<h2>1. Data structure<a class="headerlink" href="#data-structure" title="Link to this heading">¶</a></h2>
<p>The ICASSP 2024 Cadenza Challenge dataset is based on the MUSDB18-HQ dataset.
To download the data, please visit <a class="reference external" href="https://cadenzachallenge.org/docs/icassp_2024/take_part/download">Download data and software</a>
webpage.</p>
<p>The data is split into four packages: <code class="docutils literal notranslate"><span class="pre">cad_icassp_2024_core.v1.1.tgz</span></code>,
<code class="docutils literal notranslate"><span class="pre">cad_icassp_2024_train.v1.0.tgz</span></code>, and <code class="docutils literal notranslate"><span class="pre">cad_icassp_2024_medleydb.tgz</span></code>.</p>
<p>Unpack packages under the same root directory using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-xvzf<span class="w"> </span>&lt;PACKAGE_NAME&gt;
</pre></div>
</div>
<section id="necessary-data">
<h3>1.1 Necessary data<a class="headerlink" href="#necessary-data" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Core</strong> contains the metadata and HRTFs signals to generate the ICASSP 2024 dataset.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
├───audio
|   └───hrtf (336 kB)
|       |  BTE_fr-VP_E1-n22.5.wav
|       |  BTE_fr-VP_E1-n30.0.wav
|       |  ...
|
└───metadata  (328 kB)
    |  gains.json
    |  head_positions.json
    |  listeners.train.json
    |  listeners.valid.json
    |  musdb18.train.json
    |  scene_listeners.train.json
    |  scenes.train.json
    |  ...
</pre></div>
</div>
<ul class="simple">
<li><p><strong>train</strong> contains the MUSDB18 train split signals to generate the ICASSP 2024 dataset.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───audio
    └───music (22 GB)
        └─── Train
             ├─── A Classic Education - NightOwl
             |    | Bass.wav
             |    | Drums.wav
             |    | Other.wav
             |    | Vocals.wav
             |    | Mixture.wav
             ├─── ...
</pre></div>
</div>
</section>
<section id="additional-optional-data">
<h3>1.2 Additional optional data<a class="headerlink" href="#additional-optional-data" title="Link to this heading">¶</a></h3>
<p>If you need additional music data for training your model, please restrict to the use of
<a class="reference external" href="https://medleydb.weebly.com/">MedleyDB</a> [<a class="reference internal" href="#references"><span class="xref myst">5</span></a>] [<a class="reference internal" href="#references"><span class="xref myst">6</span></a>],
<a class="reference external" href="https://labsites.rochester.edu/air/resource.html">BACH10</a> [7] and <a class="reference external" href="https://github.com/mdeff/fma">FMA-small</a> [7].</p>
<p><strong>Keeping the augmentation data restricted to these datasets will ensure that the evaluation is fair for all participants</strong>.</p>
<ul class="simple">
<li><p><strong>MedleyDB</strong> contains both MedleyDB versions 1 [<a class="reference internal" href="#references"><span class="xref myst">5</span></a>] and 2 [<a class="reference internal" href="#references"><span class="xref myst">6</span></a>] datasets.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cadenza_data
└───audio
    └───MedleyDB (164 GB)
        ├───Audio
        └───Metadata
</pre></div>
</div>
<ul class="simple">
<li><p><strong>BACH10</strong> [<a class="reference internal" href="#references"><span class="xref myst">7</span></a>].</p></li>
</ul>
<p>Bach10 dataset can be downloaded from
<a class="reference external" href="https://cadenzachallenge.org/docs/icassp_2024/take_part/download#b1-download-the-packages">Download data and software</a>
on the Challenge website.</p>
<ul class="simple">
<li><p><strong>FMA Small</strong> contains the FMA small subset of the FMA dataset [<a class="reference internal" href="#references"><span class="xref myst">8</span></a>].</p></li>
</ul>
<p>FMA small dataset can be downloaded from
<a class="reference external" href="https://cadenzachallenge.org/docs/icassp_2024/take_part/download#b1-download-the-packages">Download data and software</a>
on the Challenge website.</p>
<p>Tracks from the FMA small dataset are not included in the MUSDB18-HQ.
This dataset does not provide independent stems but only the full mix.
However, it can be used to train an unsupervised model to better initialise a supervised model.</p>
</section>
</section>
<section id="baseline">
<h2>2. Baseline<a class="headerlink" href="#baseline" title="Link to this heading">¶</a></h2>
<p>In the <code class="docutils literal notranslate"><span class="pre">baseline/</span></code> folder, we provide code for running the baseline enhancement system and performing the objective evaluation.
Note that we use <a class="reference external" href="https://hydra.cc/docs/intro/">hydra</a> for config handling.</p>
<section id="enhancement">
<h3>2.1 Enhancement<a class="headerlink" href="#enhancement" title="Link to this heading">¶</a></h3>
<p>The baseline enhance takes an out-of-the-box source separation model and estimates
the VDBO (vocals, drums, bass and others) stems for each song-listener pair.</p>
<p>For each estimated stem, the baseline applies the gains and remix the signal.
A simple NAL-R [2] fitting amplification is applied to the final remix</p>
<p>The baseline offers 2 source separation options:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/facebookresearch/demucs">Hybrid Demucs</a> [<a class="reference internal" href="#references"><span class="xref myst">1</span></a>]  distributed on <a class="reference external" href="https://pytorch.org/audio/main/tutorials/hybrid_demucs_tutorial.html">TorchAudio</a></p></li>
<li><p><a class="reference external" href="https://github.com/sigsep/open-unmix-pytorch">Open-Unmix</a> [<a class="reference internal" href="#references"><span class="xref myst">2</span></a>]  distributed through Pytorch hub.</p></li>
</ol>
<p>To run the baseline enhancement system first, make sure that <code class="docutils literal notranslate"><span class="pre">paths.root</span></code> in <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> points to
where you have installed the Cadenza data.
You can also define your own <code class="docutils literal notranslate"><span class="pre">path.exp_folder</span></code> to store enhanced
signals and evaluated results.</p>
<p>Then run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py
</pre></div>
</div>
<p>Alternatively, you can provide the root variable on the command line, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>path.root<span class="o">=</span>/Volumes/data/cadenza_data
</pre></div>
</div>
<p>To get a full list of the parameters, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>--help
</pre></div>
</div>
<p>The folder <code class="docutils literal notranslate"><span class="pre">enhanced_signals</span></code> will appear in the <code class="docutils literal notranslate"><span class="pre">exp</span></code> folder.</p>
</section>
<section id="evaluation">
<h3>2.2 Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluate.py</span></code> simply takes the signals stored in <code class="docutils literal notranslate"><span class="pre">enhanced_signals</span></code> and computes the HAAQI [<a class="reference internal" href="#references"><span class="xref myst">3</span></a>] scores</p>
<p>To run the evaluation stage, make sure that <code class="docutils literal notranslate"><span class="pre">path.root</span></code> is set in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file and then run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py
</pre></div>
</div>
<p>A csv file containing the left and right channels HAAQI scores and the mean of both will be generated in the <code class="docutils literal notranslate"><span class="pre">path.exp_folder</span></code>.</p>
<p>To check the HAAQI code, see <a class="reference external" href="https://github.com/claritychallenge/clarity/blob/main/clarity/evaluator/haaqi/haaqi.py">here</a>.</p>
<p>Please note: you will not get identical HAAQI scores for the same signals if the random seed is not defined
(in the given recipe, the random seed for each signal is set as the last eight digits of the song md5).
As there are random noises generated within HAAQI, but the differences should be sufficiently small.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>[1] Défossez, A. “Hybrid Spectrogram and Waveform Source Separation”. Proceedings of the ISMIR 2021 Workshop on Music Source Separation. <a class="reference external" href="https://arxiv.org/abs/2111.03600">doi:10.48550/arXiv.2111.03600</a></p></li>
<li><p>[2] Stöter, F. R., Liutkus, A., Ito, N., Nakashika, T., Ono, N., &amp; Mitsufuji, Y. (2019). “Open-Unmix: A Reference Implementation for Music Source Separation”. Journal of Open Source Software, 4(41), 1667. <a class="reference external" href="https://doi.org/10.21105/joss.01667">doi:10.21105/joss.01667</a></p></li>
<li><p>[3] Byrne, Denis, and Harvey Dillon. “The National Acoustic Laboratories’(NAL) new procedure for selecting the gain and frequency response of a hearing aid.” Ear and hearing 7.4 (1986): 257-265. <a class="reference external" href="https://doi.org/10.1097/00003446-198608000-00007">doi:10.1097/00003446-198608000-00007</a></p></li>
<li><p>[4] Kates J M, Arehart K H. “The Hearing-Aid Audio Quality Index (HAAQI)”. IEEE/ACM transactions on audio, speech, and language processing, 24(2), 354–365. <a class="reference external" href="https://doi.org/10.1109%2FTASLP.2015.2507858">doi:10.1109/TASLP.2015.2507858</a></p></li>
<li><p>[5] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, “MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research”, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014. <a class="reference external" href="https://archives.ismir.net/ismir2014/paper/000322.pdf">pdf</a></p></li>
<li><p>[6] Rachel M. Bittner, Julia Wilkins, Hanna Yip and Juan P. Bello, “MedleyDB 2.0: New Data and a System for Sustainable Data Collection” Late breaking/demo extended abstract, 17th International Society for Music Information Retrieval (ISMIR) conference, August 2016. <a class="reference external" href="https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/bittner-medleydb.pdf">pdf</a></p></li>
<li><p>[7] Zhiyao Duan, Bryan Pardo and Changshui Zhang, “Multiple fundamental frequency estimation by modeling spectral peaks and non-peak regions,” IEEE Trans. Audio Speech  Language Process., vol. 18, no. 8, pp. 2121-2133, 2010. <a class="reference external" href="https://doi.org/10.1109/TASL.2010.2042119">doi:10.1109/TASL.2010.2042119</a></p></li>
<li><p>[8] Defferrard, M., Benzi, K., Vandergheynst, P., &amp; Bresson, X. (2016). “FMA: A dataset for music analysis”. arXiv preprint arXiv:1612.01840. <a class="reference external" href="https://doi.org/10.48550/arXiv.1612.01840">doi:10.48550/arXiv.1612.01840</a></p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">Project name not set</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes_doc.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CONTRIBUTING.html">Contributing to pyClarity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../clarity.html">clarity package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes.html">recipes package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../../../_sources/recipes/cad_icassp_2024/baseline/README.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>