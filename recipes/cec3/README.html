

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The 3rd Clarity Enhancement Challenge (CEC3) &mdash; pyClarity 0.7.1.post21 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5cc28f93"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            pyClarity
              <img src="../../_static/challenges.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">pyClarity</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The 3rd Clarity Enhancement Challenge (CEC3)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/recipes/cec3/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-3rd-clarity-enhancement-challenge-cec3">
<h1>The 3rd Clarity Enhancement Challenge (CEC3)<a class="headerlink" href="#the-3rd-clarity-enhancement-challenge-cec3" title="Link to this heading"></a></h1>
<p>Clarity challenge code for the 3rd Clarity Enhancement Challenge.</p>
<p>For more information please visit the <a class="reference external" href="https://claritychallenge.org/docs/cec3/cec3_intro">challenge website</a>.</p>
<p>Clarity tutorials are <a class="reference external" href="https://claritychallenge.github.io/clarity_CC_doc/tutorials">now available</a>. The tutorials introduce the Clarity installation, how to interact with Clarity metadata, and also provide examples of baseline systems and evaluation tools.</p>
<section id="data-structure">
<h2>Data structure<a class="headerlink" href="#data-structure" title="Link to this heading"></a></h2>
<p>The 3rd Clarity Enhancement Challenge consists of three separate tasks each with its own training and evaluation data. Details for how to obtain the data can be found on the <a class="reference external" href="https://claritychallenge.org/docs/cec3/cec3_data">challenge website</a>.</p>
<p>The data is distributed as one or more separate packages in <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code> format.</p>
<p>Unpack all packages under the same root directory using</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-xvzf<span class="w"> </span>&lt;PACKAGE_NAME&gt;
</pre></div>
</div>
<p>The initially released data is in the package <code class="docutils literal notranslate"><span class="pre">clarity_CEC3_data.v1_0.tar.gz</span></code> and has the following structure:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>clarity_CEC3_data
|── manifest
|── task1
|   |── clarity_data
|   |   |── dev
|   |   |   |── scenes
|   |   |   └── speaker_adapt
|   |   |── metadata
|   |   └── train
|   └── hrir
|       └── HRIRs_MAT
|── task2
|   └── clarity_data
|       |── dev
|       |   |── interferers
|       |   |── scenes
|       |   |── speaker_adapt
|       |   └── targets
|       |── metadata
|       └── train
|           |── interferers
|           |── scenes
|           └── targets
└── task3
</pre></div>
</div>
</section>
<section id="baseline">
<h2>Baseline<a class="headerlink" href="#baseline" title="Link to this heading"></a></h2>
<p>In the `baseline/’ folder, we provide code for running the baseline enhancement system and performing the objective evaluation. The same system can be used for all three tasks by setting the configuration appropriately.</p>
<p>The scripts are controlled by three variables.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">task</span></code> - The task to evaluate. This can be <code class="docutils literal notranslate"><span class="pre">task1</span></code>, <code class="docutils literal notranslate"><span class="pre">task2</span></code> or <code class="docutils literal notranslate"><span class="pre">task3</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">path.root</span></code> - The root directory where you clarity data is stored.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">path.exp</span></code> - A directory that will be used to store intermediate files and the final evaluation results.</p></li>
</ul>
<p>These can be set in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file or provided on the command line. In the following they are being set on the command line.</p>
<section id="enhancement">
<h3>Enhancement<a class="headerlink" href="#enhancement" title="Link to this heading"></a></h3>
<p>The baseline enhancement simply takes the 6-channel hearing aid inputs and reduces this to a stereo hearing aid output by passing through the ‘front’ microphone signal of the left and right ear.</p>
<p>The stereo pair is then passed through a provided hearing aid amplification stage using a NAL-R [<a class="reference internal" href="#references"><span class="xref myst">1</span></a>] fitting amplification and a simple automatic gain compressor. The amplification is determined by the audiograms defined by the scene-listener pairs in <code class="docutils literal notranslate"><span class="pre">clarity_data/metadata/scenes_listeners.dev.json</span></code> for the development set. After amplification, the evaluate function calculates the better-ear HASPI  [<a class="reference internal" href="#references"><span class="xref myst">2</span></a>].</p>
<p>To run the baseline enhancement system use, first set the <code class="docutils literal notranslate"><span class="pre">task</span></code>, <code class="docutils literal notranslate"><span class="pre">path.root</span></code> and <code class="docutils literal notranslate"><span class="pre">path.exp</span></code> variables in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file and then run,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py
</pre></div>
</div>
<p>Alternatively, you can provide the task and paths on the command line, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span><span class="nv">task</span><span class="o">=</span>task1<span class="w"> </span>path.root<span class="o">=</span>/Users/jon/clarity_CEC3_data<span class="w"> </span>path.exp<span class="o">=</span>/Users/jon/exp
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">/Users/jon</span></code> is replaced with the path to the root of the clarity data and the experiment folder.</p>
<p>The folders <code class="docutils literal notranslate"><span class="pre">enhanced_signals</span></code>  and <code class="docutils literal notranslate"><span class="pre">amplified_signals</span></code> will appear in the <code class="docutils literal notranslate"><span class="pre">exp</span></code> folder. Note, the experiment folder will be created if it does not already exist.</p>
</section>
<section id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h3>
<p>The evaluate script computes the HASPI scores for the signals stored in the <code class="docutils literal notranslate"><span class="pre">amplified_signals</span></code> folder. The script will read the scene-listener pairs from the development set and calculate the HASPI score for each pair. The final score is the mean HASPI score across all pairs. It can be run as,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span><span class="nv">task</span><span class="o">=</span>task1<span class="w"> </span>path.root<span class="o">=</span>/Users/jon/clarity_CEC3_data<span class="w"> </span>path.exp<span class="o">=</span>/Users/jon/exp
</pre></div>
</div>
<p>The full evaluation set is 7500 scene-listener pairs and will take a long time to run, i.e., around 8 hours on a MacBook Pro. A standard small set which uses 1/15 of the data has been defined. This takes around 30 minutes to evaluate and can be run with,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span><span class="nv">task</span><span class="o">=</span>task1<span class="w"> </span>path.root<span class="o">=</span>/Users/jon/clarity_CEC3_data<span class="w"> </span>path.exp<span class="o">=</span>/Users/jon/exp<span class="w"> </span>evaluate.small_test<span class="o">=</span>True
</pre></div>
</div>
<p>Alternatively, see the section below, ‘Running with multiple threads’, for how to run with multiple threads or on an HPC system.</p>
<p>The evaluation script will generate a CSV file containing the HASPI scores for each sample. This can be found in <code class="docutils literal notranslate"><span class="pre">&lt;path.exp&gt;/scores</span></code></p>
</section>
<section id="reporting-results">
<h3>Reporting results<a class="headerlink" href="#reporting-results" title="Link to this heading"></a></h3>
<p>Once the evaluation script has finished running, the final result can be reported with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>report_score.py<span class="w"> </span><span class="nv">task</span><span class="o">=</span>task1<span class="w"> </span>path.root<span class="o">=</span>/Users/jon/clarity_CEC3_data<span class="w"> </span>path.exp<span class="o">=</span>/Users/jon/exp
</pre></div>
</div>
<p>Or if you have run the small evaluation</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>report_score.py<span class="w"> </span><span class="nv">task</span><span class="o">=</span>task1<span class="w"> </span>path.root<span class="o">=</span>/Users/jon/clarity_CEC3_data<span class="w"> </span>path.exp<span class="o">=</span>/Users/jon/exp<span class="w"> </span>evaluate.small_test<span class="o">=</span>True
</pre></div>
</div>
<p>The scores for Task 1 and Task 2 should be as follows.</p>
<p>Task 1</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Evaluation set size: 7500
Mean HASPI score: 0.22178678134846783

                 SNR     haspi
SNR
(-12, -9] -10.498088  0.052545
(-9, -6]   -7.541468  0.080589
(-6, -3]   -4.477046  0.143096
(-3, 0]    -1.432494  0.239527
(0, 3]      1.470118  0.352110
(3, 6]      4.492380  0.477001
</pre></div>
</div>
<p>Task 2</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Evaluation set size: 7500
Mean HASPI score: 0.18643217215546573

                 SNR     haspi
SNR
(-12, -9] -10.545927  0.034330
(-9, -6]   -7.552687  0.055647
(-6, -3]   -4.538335  0.096237
(-3, 0]    -1.455963  0.178413
(0, 3]      1.434074  0.296364
(3, 6]      4.507484  0.432177
</pre></div>
</div>
</section>
</section>
<section id="tips">
<h2>Tips<a class="headerlink" href="#tips" title="Link to this heading"></a></h2>
<section id="configuring-with-hydra">
<h3>Configuring with Hydra<a class="headerlink" href="#configuring-with-hydra" title="Link to this heading"></a></h3>
<p>The code is using <a class="reference external" href="https://hydra.cc">Hydra</a> for configuration management. This allows for easy configuration of the system. The configuration file is <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> in the <code class="docutils literal notranslate"><span class="pre">baseline</span></code> folder. The task, root and exp variables can be set in this file to avoid having to set them on every command line. Simply replace the <code class="docutils literal notranslate"><span class="pre">???</span></code> entries with the appropriate values.</p>
<p>You can make alternative configurations and store them in separate <code class="docutils literal notranslate"><span class="pre">yaml</span></code> files. These can then be used to override the default configuration, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>python<span class="w"> </span>report_score.py<span class="w"> </span>--config-name<span class="w"> </span>my_task1_config.yaml
</pre></div>
</div>
<p>You can get help on any of the commands with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>--help
</pre></div>
</div>
<p>And specific help on Hydra usage with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>enhance.py<span class="w"> </span>--hydra-help
</pre></div>
</div>
</section>
<section id="running-with-multiple-threads">
<h3>Running with multiple threads<a class="headerlink" href="#running-with-multiple-threads" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluate.py</span></code> script can be sped up by running with multiple processes, i.e. each process will evaluate a separate block of scenes and generate its own csv file. The <code class="docutils literal notranslate"><span class="pre">report_score.py</span></code> script will then combine these csv files to produce a single result.</p>
<p>To do this we can use the Hydra <code class="docutils literal notranslate"><span class="pre">--multirun</span></code> flag and set multiple values for <code class="docutils literal notranslate"><span class="pre">evaluate.first_scene</span></code>. For example, to run with 4 threads we can split the 7500 scenes into 4 blocks of 1875 scenes each and run with,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span>evaluate.first_scene<span class="o">=</span><span class="s2">&quot;0,1875,3750,5625&quot;</span><span class="w"> </span>evaluate.n_scenes<span class="o">=</span><span class="m">1875</span><span class="w"> </span>--multirun
</pre></div>
</div>
<p>Hydra has a Python like system for specifying ranges, so the above command is equivalent to</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w">  </span>evaluate.first_scene<span class="o">=</span><span class="s2">&quot;range(0,7500,1875) evaluate.n_scenes=1875 --multirun</span>
</pre></div>
</div>
<p>If we wanted to split into jobs with just 100 scenes per job we could use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span>evaluate.first_scene<span class="o">=</span><span class="s2">&quot;range(0,7500,100)&quot;</span><span class="w"> </span>evaluate.n_scenes<span class="o">=</span><span class="m">500</span><span class="w"> </span>--multirun
</pre></div>
</div>
<p>Hydra will launch these job using configuration that can be found in <code class="docutils literal notranslate"><span class="pre">hydra/launcher/cec3_submitit_local.yaml</span></code>.</p>
<p>The same approach can be used to run jobs on a SLURM cluster using configuration in <code class="docutils literal notranslate"><span class="pre">hydra/launcher/cec3_submitit_slurm.yaml</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span>hydra/launcher<span class="o">=</span>cec3_submitit_slurm<span class="w"> </span>evaluate.first_scene<span class="o">=</span><span class="s2">&quot;range(0,7500,100)&quot;</span><span class="w"> </span>evaluate.n_scenes<span class="o">=</span><span class="m">100</span><span class="w"> </span>--multirun
</pre></div>
</div>
<p>!!!Note In the examples above it is assumed that the <code class="docutils literal notranslate"><span class="pre">task</span></code>, <code class="docutils literal notranslate"><span class="pre">path.root</span></code> and <code class="docutils literal notranslate"><span class="pre">path.exp</span></code> variables are set in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file.</p>
<p>!!!Note Hydra has plugin support for other job launchers. See the <a class="reference external" href="https://hydra.cc/docs/intro/">Hydra documentation for more information</a>.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>[1] Byrne, Denis, and Harvey Dillon. “The National Acoustic Laboratories’(NAL) new procedure for selecting the gain and frequency response of a hearing aid.” Ear and hearing 7.4 (1986): 257-265.</p></li>
<li><p>[2] Kates J M, Arehart K H. The hearing-aid speech perception index (HASPI) J. Speech Communication, 2014, 65: 75-93.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2025, pyClarity authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>